{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Baseline for experiments.ipynb","provenance":[],"collapsed_sections":["T8faoY_lpDhz","7Re-1P2y5D7C"],"authorship_tag":"ABX9TyOwNVVKyA1A/hAdFsA5ts23"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup "],"metadata":{"id":"T8faoY_lpDhz"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7TWLTC1DpBG3","executionInfo":{"status":"ok","timestamp":1640727514318,"user_tz":-120,"elapsed":14691,"user":{"displayName":"Martti Praks","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12475164514759227724"}},"outputId":"a66bee03-abcc-413a-b94c-dfa3ef5548c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym_minigrid in /usr/local/lib/python3.7/dist-packages (1.0.2)\n","Requirement already satisfied: gym>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from gym_minigrid) (0.17.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from gym_minigrid) (1.19.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.9.6->gym_minigrid) (1.5.0)\n","Collecting cloudpickle<1.7.0,>=1.2.0\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.9.6->gym_minigrid) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.9.6->gym_minigrid) (0.16.0)\n","Installing collected packages: cloudpickle\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 2.0.0\n","    Uninstalling cloudpickle-2.0.0:\n","      Successfully uninstalled cloudpickle-2.0.0\n","Successfully installed cloudpickle-1.6.0\n","Requirement already satisfied: stable_baselines3 in /usr/local/lib/python3.7/dist-packages (1.3.0)\n","Requirement already satisfied: gym<0.20,>=0.17 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (0.17.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.19.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.10.0+cu111)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.6.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable_baselines3) (1.1.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable_baselines3) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym<0.20,>=0.17->stable_baselines3) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.20,>=0.17->stable_baselines3) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->stable_baselines3) (3.10.0.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (3.0.6)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable_baselines3) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->stable_baselines3) (2018.9)\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\u001b[0m\n","Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.9)\n","Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.2.0)\n","Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n","Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.24)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n","Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n"]}],"source":["!pip install gym_minigrid\n","!pip install stable_baselines3\n","!pip install --upgrade --quiet cloudpickle pickle5\n","!pip install wandb"]},{"cell_type":"code","source":["from gym_minigrid.wrappers import *\n","from gym_minigrid.minigrid import *\n","import gym\n","\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.vec_env import SubprocVecEnv\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.utils import set_random_seed\n","from stable_baselines3.common.evaluation import evaluate_policy\n","\n","import torch.nn as nn\n","import numpy as np\n","\n","import itertools\n","import random\n","\n","import matplotlib.pyplot as plt\n","\n","import wandb\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.vec_env import VecVideoRecorder\n","from wandb.integration.sb3 import WandbCallback\n","\n","%matplotlib notebook\n","\n","custom_objects = {\n","    \"lr_schedule\": lambda x: .003,\n","    \"clip_range\": lambda x: .02\n","}\n","\n","import imageio as iio\n","import os"],"metadata":{"id":"DIL_KskopHhR","executionInfo":{"status":"ok","timestamp":1640727521539,"user_tz":-120,"elapsed":7246,"user":{"displayName":"Martti Praks","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12475164514759227724"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Create gif out of environment and agent. Possibly upload to W&B\n","# gif_name: [\"hacked\", \"initial_changed_env\"] (have mappings in W&B) or custom\n","def save_gif(gif_name, max_frames, max_episodes, model, env, log_to_wb = True):\n","\n","  path = 'gif'\n","  # Check whether the specified path exists or not\n","  isExist = os.path.exists(path)\n","\n","  if not isExist:    \n","    # Create a new directory because it does not exist \n","    os.makedirs(path)\n","\n","  images = []\n","  gif_path = gif_name + \".gif\"\n","  frames_path = path+\"/\"+gif_name+\"{j}.jpg\"\n","\n","  j = 0\n","  obs = env.reset()\n","  img = env.render(mode='rgb_array')\n","  for i in range(max_episodes):\n","      obs = env.reset()\n","      while True and j < max_frames:\n","        action, _ = model.predict(obs)\n","        obs, r, done ,_ = env.step(action)\n","        fig,(ax1) = plt.subplots(1,1, figsize=(5, 5));\n","        image1 = ax1.imshow(env.render(\"rgb_array\"))\n","        ax1.set_title(f\"Hacked obs: Action = {action}, Reward = {r}, \\n Done = {done}\")\n","        plt.savefig(frames_path.format(j=j))\n","        plt.cla()\n","        j=j+1 \n","        if done: \n","          break\n","  with iio.get_writer(gif_path, mode='I') as writer:\n","      for j in range(max_frames):\n","          try:\n","            writer.append_data(iio.imread(frames_path.format(j=j)))\n","          except OSError as e:\n","            None\n","  if log_to_wb == True:\n","    wandb.log({gif_name: wandb.Video(gif_path)})"],"metadata":{"id":"ANYrJlbHpL4B","executionInfo":{"status":"ok","timestamp":1640727521540,"user_tz":-120,"elapsed":28,"user":{"displayName":"Martti Praks","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12475164514759227724"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["#Vanilla environment for first agent\n","class EmptyBallRoom(MiniGridEnv):\n","    def __init__(self, size=9, ball_color=\"green\", tile_size=8):\n","        self.ball_color = ball_color\n","        self.tile_size = tile_size\n","        super().__init__(\n","            grid_size=size,\n","            max_steps=4*size*size,\n","            # Set this to True for maximum speed\n","            see_through_walls=True\n","        )\n","        self.observation_space = spaces.Box(\n","            low=0,\n","            high=255,\n","            shape=(size * tile_size, size * tile_size, 3),\n","            dtype='uint8'\n","        )\n","    \n","    def reset(self):\n","        super().reset()\n","        rgb_observation = self.render(\"rgb_image\", highlight=False, tile_size=self.tile_size)\n","        return rgb_observation\n","        \n","    def _gen_grid(self, width, height):\n","        # Create an empty grid\n","        self.grid = Grid(width, height)\n","        # Generate the surrounding walls\n","        self.grid.wall_rect(0, 0, width, height)\n","\n","        # Place the agent\n","        # self.agent_pos = (1, 1)\n","        # self.agent_dir=0\n","        self.place_agent()\n","        \n","        # Place object\n","        self.ball = Ball(self.ball_color)\n","        # self.put_obj(self.ball, width-2, height-2)\n","        self.place_obj(self.ball)\n","        self.mission = f\"Pick up {self.ball_color} {self.ball.type}\"\n","        \n","    def step(self, action):\n","        obs, reward, done, info = super().step(action)\n","\n","        if action == self.actions.pickup:\n","            if self.carrying and self.carrying == self.ball:\n","                reward = self._reward()\n","                done = True\n","        \n","        rgb_observation = self.render(\"rgb_image\", highlight=False, tile_size=self.tile_size)\n","        \n","        return rgb_observation, reward, done, info"],"metadata":{"id":"RLiMCalDpLz_","executionInfo":{"status":"ok","timestamp":1640727521541,"user_tz":-120,"elapsed":27,"user":{"displayName":"Martti Praks","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12475164514759227724"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#Wrapper to change starting ball color randomly\n","class RandomBallColorWrapper(gym.core.Wrapper):\n","    \n","    def __init__(self, env):\n","        super().__init__(env)\n","    \n","    def reset(self):\n","        self.env.ball_color = random.sample([\"red\", \"green\", \"blue\", \"yellow\", \"purple\"], 1)[0]\n","        return super().reset()"],"metadata":{"id":"DrKf_w0tpLxC","executionInfo":{"status":"ok","timestamp":1640727521542,"user_tz":-120,"elapsed":27,"user":{"displayName":"Martti Praks","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12475164514759227724"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Hacked environment wrapper\n","class HackedEnvironment(gym.core.Wrapper):\n","    COLOR_TO_INDEX = {\"red\": 0, \"green\": 1, \"blue\": 2, \"purple\": 3, \"yellow\": 4}\n","    INDEX_TO_COLOR = {0: \"red\", 1: \"green\", 2: \"blue\", 3: \"purple\", 4: \"yellow\"}\n","    \n","    \n","    def __init__(self, env, first_agent):\n","        self.first_agent = first_agent\n","        super().__init__(env)\n","        self.action_space = spaces.Discrete(len(self.COLOR_TO_INDEX))\n","        \n","        self.transformed_obs = np.array([])\n","        self.original_obs = np.array([])\n","        \n","        self.transformation_action = None\n","        self.environment_action = None\n","        \n","        self.reset()\n","        \n","    def reset(self):\n","        self.env.reset()\n","        self.original_ball_color = self.env.ball.color\n","        self.original_obs = self.transformed_obs = self.env.render(\n","            \"rgb_image\",\n","            highlight=False,\n","            tile_size=self.env.tile_size\n","        )\n","        self.transformation_action\n","        return self.original_obs\n","\n","    \n","    def step(self, first_action):\n","        \"\"\"\n","        This agent changes some random color from predefined colors to goal color. If that color does\n","        not exist then nothing changes.\n","        \n","        1) 2nd agent makes an action and changes ball's color\n","        2) Render changed environment (change ball color in environment)\n","            which will be inserted to first agent's model\n","        3) First agent returns action and perform it on real environment\n","        4) Change ball color back to real ball color\n","        \"\"\"\n","        # ball_pos = self.env.ball.cur_pos\n","        self.transformation_action = first_action\n","        if self.ball.color == self.INDEX_TO_COLOR[first_action]:\n","            self.ball.color = \"green\"\n","            \n","        self.transformed_obs = self.env.render(\"rgb_image\", highlight=False, tile_size=self.env.tile_size)          \n","        second_action, _ = self.first_agent.predict(self.transformed_obs)\n","        self.environment_action = second_action\n","        self.transformed_obs = self.env.render(\"rgb_image\", highlight=False, tile_size=self.env.tile_size)\n","        \n","        \n","        # Change color back to original\n","        self.ball.color = self.original_ball_color        \n","        self.original_obs, reward, done, info = self.env.step(second_action)\n","        \n","        return self.original_obs, reward, done, info\n","        \n","    def render(self, pov=\"agent\", highlight=False, **kwargs):\n","        \"\"\"\n","        pov : agent / original\n","        \"\"\"\n","        if pov==\"agent\":\n","            if self.transformation_action is None:\n","                return self.render(\"original\", highlight=highlight, **kwargs)\n","            if self.ball.color == self.INDEX_TO_COLOR[self.transformation_action]:\n","                self.ball.color = \"green\"\n","            observation = self.env.render(highlight=highlight, **kwargs)\n","            self.ball.color = self.original_ball_color\n","        else:\n","            observation = self.env.render(highlight=highlight, **kwargs)\n","\n","        return observation"],"metadata":{"id":"0u5tfFTDpLtJ","executionInfo":{"status":"ok","timestamp":1640727521904,"user_tz":-120,"elapsed":388,"user":{"displayName":"Martti Praks","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12475164514759227724"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Experiments\n","\n","Sets up following experiments: \n","\n","\n","1.   train_and_log_baseline - trains 1st agent and logs related data to W&B\n","2.   validate_baseline - validates 1st agent on the second environment. \n","3.   train_2nd_agent_and_validate - trains second agent and validates it on second environment. \n","\n","Note: only experiments 2 and 3 are assumed to get changed with different environment changes. They also need the first agent to be provided for loading. \n"],"metadata":{"id":"x_zbFEW1pZAz"}},{"cell_type":"markdown","source":["## Experiment definitions"],"metadata":{"id":"7Re-1P2y5D7C"}},{"cell_type":"code","source":["def train_and_log_baseline(trial_description, trial_name, config):\n","  ### Initiate W&B connection\n","  project_name = \"learning-by-hacking\"\n","  run = wandb.init(\n","    project=project_name,\n","    entity=\"learning-by-hacking\",\n","    name = trial_name,\n","    config=config,\n","    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n","    monitor_gym=True,  # auto-upload the videos of agents playing the game\n","    save_code=True,  # optional\n","  )\n","\n","  ### Create env\n","  first_env_vector = make_vec_env(EmptyBallRoom, \n","                                n_envs=12, \n","                                vec_env_cls=SubprocVecEnv, \n","                                env_kwargs= {\n","                                  \"size\": config[\"ROOM_SIZE\"], \n","                                  \"ball_color\": config[\"BALL_COLOR\"], \n","                                  \"tile_size\": config[\"TILE_SIZE\"]\n","                                  }, \n","                                monitor_dir=\"env_monitor\")\n","  first_env_vector = VecVideoRecorder(first_env_vector, f\"videos/{run.id}\", record_video_trigger=lambda x: x % 2000 == 0, video_length=200) # for video loading to W&B\n","\n","  policy_kwargs = dict(activation_fn=nn.ReLU)\n","  first_agent = PPO(\n","    config[\"policy_type\"], \n","    first_env_vector,\n","    policy_kwargs=policy_kwargs,\n","    verbose=1,\n","    tensorboard_log=\"TB_first_agent\",\n","    ent_coef = 0\n","    )\n","  first_agent.learn(total_timesteps=config[\"total_timesteps\"], \n","                    callback=WandbCallback( #callback for W&B\n","                                          model_save_path=f\"models/{run.id}\",\n","                                          verbose=2,\n","                                          ),\n","                    )\n","  first_agent.save(\"first_agent\")\n","\n","  ### Validate\n","  test_env = EmptyBallRoom(ball_color = config[\"BALL_COLOR\"], size=config[\"ROOM_SIZE\"])\n","\n","  n_val_episodes = 100\n","  mean_reward, std_reward = evaluate_policy(first_agent, test_env, n_eval_episodes=n_val_episodes)\n","  wandb.log({'val_mean_reward': mean_reward, 'val_std_reward': std_reward}) #log mean reward\n","  save_gif(\"First agent on first environment\", 100, 10, first_agent, test_env, log_to_wb = True) #log gif to W&B\n","\n","  wandb.finish() \n"],"metadata":{"id":"zeucMANcpYV9","executionInfo":{"status":"ok","timestamp":1640727521905,"user_tz":-120,"elapsed":12,"user":{"displayName":"Martti Praks","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12475164514759227724"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def validate_baseline(trial_description, trial_name, config, modelName = \"first_agent\", randomColor = False):\n","  ### Initiate W&B connection\n","  project_name = \"learning-by-hacking\"\n","  run = wandb.init(\n","    project=project_name,\n","    entity=\"learning-by-hacking\",\n","    name = trial_name,\n","    config=config,\n","    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n","    monitor_gym=True,  # auto-upload the videos of agents playing the game\n","    save_code=True,  # optional\n","  )\n","\n","  ### Load agent\n","  first_agent = PPO.load(modelName, custom_objects=custom_objects)\n","  print(\"loaded:\", \"gamma =\", first_agent.gamma, \"n_steps =\", first_agent.n_steps)\n","\n","  ############################################################################################################## \n","  # Only this section is expected to get changed per different experiments\n","\n","  ### choose random color or not\n","  second_env_kwargs = {\"size\": config[\"ROOM_SIZE\"], \"ball_color\": config[\"2ND_BALL_COLOR\"], \"tile_size\": config[\"TILE_SIZE\"]} \n","  if randomColor == False:\n","    second_env = EmptyBallRoom(**second_env_kwargs) # Note: this is relevant for validation as well\n","    \n","  else:\n","    second_env = RandomBallColorWrapper(EmptyBallRoom(**second_env_kwargs))\n","\n","  ############################################################################################################## \n","  \n","  ### Validate\n","  n_val_episodes = 100\n","  mean_reward, std_reward = evaluate_policy(first_agent, second_env, n_eval_episodes=n_val_episodes)\n","  wandb.log({'val_mean_reward': mean_reward, 'val_std_reward': std_reward}) #log mean reward\n","  save_gif(\"First agent on second environment\", 100, 10, first_agent, second_env, log_to_wb = True) #log gif to W&B\n","\n","  wandb.finish() \n"],"metadata":{"id":"JAdvaiV5yEuS","executionInfo":{"status":"ok","timestamp":1640727521906,"user_tz":-120,"elapsed":12,"user":{"displayName":"Martti Praks","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12475164514759227724"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def train_2nd_agent_and_validate(trial_description, trial_name, config, modelName = \"first_agent\", randomColor = False):\n","  ### Initiate W&B connection\n","  project_name = \"learning-by-hacking\"\n","  run = wandb.init(\n","    project=project_name,\n","    entity=\"learning-by-hacking\",\n","    name = trial_name,\n","    config=config,\n","    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n","    monitor_gym=True,  # auto-upload the videos of agents playing the game\n","    save_code=True,  # optional\n","  )\n","\n","  ### Load agent\n","  first_agent = PPO.load(modelName, custom_objects=custom_objects)\n","  print(\"loaded:\", \"gamma =\", first_agent.gamma, \"n_steps =\", first_agent.n_steps)\n","\n","  ############################################################################################################## \n","  # Only this section is expected to get changed per different experiments\n","\n","  ### choose random color or not\n","  second_env_kwargs = {\"size\": config[\"ROOM_SIZE\"], \"ball_color\": config[\"2ND_BALL_COLOR\"], \"tile_size\": config[\"TILE_SIZE\"]} \n","  if randomColor == False:\n","    second_env = EmptyBallRoom(**second_env_kwargs) # Note: this is relevant for validation as well\n","    \n","  else:\n","    second_env = RandomBallColorWrapper(EmptyBallRoom(**second_env_kwargs))\n","  \n","  ## vectorize for training\n","  second_env_vector = make_vec_env(\n","    HackedEnvironment,\n","    n_envs=2,                           # max 2 for colaboratory\n","    vec_env_cls=SubprocVecEnv,\n","    env_kwargs={\"env\": second_env, \n","                \"first_agent\": first_agent},\n","    monitor_dir=\"env_monitor\")\n","  \n","  ### train second agent\n","  policy_kwargs = dict(activation_fn=nn.ReLU)\n","  second_agent = PPO(\n","      config[\"policy_type\"], \n","      second_env_vector,\n","      gamma=0.5,\n","      policy_kwargs=policy_kwargs,\n","      verbose=1,\n","      tensorboard_log=\"second_agent_tb\",\n","      ent_coef = 0\n","  )\n","  second_agent.learn(total_timesteps=config[\"total_timesteps\"], callback=WandbCallback( #callback for W&B\n","        model_save_path=f\"models/{run.id}\", verbose=2, ),\n","        )\n","  second_agent.save(\"second_agent\")\n","\n","  ############################################################################################################## \n","  \n","  ### Validate\n","\n","  n_val_episodes = 100\n","  mean_reward, std_reward = evaluate_policy(second_agent, second_env, n_eval_episodes=n_val_episodes)\n","  wandb.log({'val_mean_reward': mean_reward, 'val_std_reward': std_reward}) #log mean reward\n","  save_gif(\"Second agent on second environment\", 100, 10, second_agent, second_env, log_to_wb = True) #log gif to W&B\n","\n","  wandb.finish() \n"],"metadata":{"id":"-WLn3VEkyErL","executionInfo":{"status":"ok","timestamp":1640727521907,"user_tz":-120,"elapsed":12,"user":{"displayName":"Martti Praks","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12475164514759227724"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Running the experiments"],"metadata":{"id":"h4dhAaiF4gV-"}},{"cell_type":"code","source":["#Experiment 1\n","trial_description = 'Training and validating first agent'\n","trial_name = '1st agent training'\n","project_name = \"learning-by-hacking\"\n","config = {\n","    \"policy_type\": \"CnnPolicy\",\n","    \"total_timesteps\": 200_000,\n","    \"env_name\": \"Baseline 1\",\n","    \"ROOM_SIZE\": 9,\n","    \"BALL_COLOR\": \"green\",\n","    \"TILE_SIZE\": 8,           # TODO not used actually?\n","    \"Trial description\": trial_description\n","}\n","\n","train_and_log_baseline(trial_description, trial_name, config)"],"metadata":{"id":"GKglih424fJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Experiment 2\n","trial_description = 'Validating first agent on new, second, environment'\n","trial_name = '1st agent validation'\n","project_name = \"learning-by-hacking\"\n","config = {\n","    \"policy_type\": \"CnnPolicy\",\n","    \"total_timesteps\": 200_000, #irrelevent here\n","    \"env_name\": \"Baseline 2\",\n","    \"ROOM_SIZE\": 9,\n","    \"BALL_COLOR\": \"green\",\n","    \"2ND_BALL_COLOR\": \"red\",\n","    \"TILE_SIZE\": 8,\n","    \"Trial description\": trial_description\n","}\n","\n","validate_baseline(trial_description, trial_name, config, modelName = \"first_agent\", randomColor = False)"],"metadata":{"id":"TVa2HBHa0ZPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Experiment 3\n","trial_description = 'Validating second agent on new, second, environment, along with logging of the training'\n","trial_name = '2nd agent training and validation'\n","project_name = \"learning-by-hacking\"\n","config = {\n","    \"policy_type\": \"CnnPolicy\",\n","    \"total_timesteps\": 200_000,\n","    \"env_name\": \"2nd environment\",\n","    \"ROOM_SIZE\": 9,\n","    \"BALL_COLOR\": \"green\",\n","    \"2ND_BALL_COLOR\": \"red\",\n","    \"TILE_SIZE\": 8,\n","    \"Trial description\": trial_description\n","}\n","\n","train_2nd_agent_and_validate(trial_description, trial_name, config, modelName = \"first_agent\", randomColor = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121},"id":"PLYH2Zt80ZLv","outputId":"344ca16f-f7cc-4a70-abfd-fa53cc90ed35"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/learning-by-hacking/learning-by-hacking/runs/yoygdjht\" target=\"_blank\">2nd agent training and validation</a></strong> to <a href=\"https://wandb.ai/learning-by-hacking/learning-by-hacking\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["loaded: gamma = 0.99 n_steps = 2048\n","Using cpu device\n","Wrapping the env in a VecTransposeImage.\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call wandb.tensorboard.patch(root_logdir=\"...\") before wandb.init\n"]},{"output_type":"stream","name":"stdout","text":["Logging to second_agent_tb/PPO_4\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"AdnZU9V_yEnr"},"execution_count":null,"outputs":[]}]}